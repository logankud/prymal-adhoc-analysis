{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import botocore\n",
    "from botocore.exceptions import BotoCoreError, ClientError, NoCredentialsError, PartialCredentialsError, ParamValidationError, WaiterError\n",
    "import loguru\n",
    "from loguru import logger\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import statistics\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY']=''    # replace with AWS creds\n",
    "os.environ['AWS_ACCESS_SECRET']= ''   # replace with AWS creds\n",
    "\n",
    "AWS_ACCESS_KEY_ID=os.environ['AWS_ACCESS_KEY']\n",
    "AWS_SECRET_ACCESS_KEY=os.environ['AWS_ACCESS_SECRET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------\n",
    "# FUNCTIONS\n",
    "# ---------------------------------------\n",
    "\n",
    "# FUNCTION TO EXECUTE ATHENA QUERY AND RETURN RESULTS\n",
    "# ----------\n",
    "\n",
    "def run_athena_query(query:str, database: str, region:str):\n",
    "\n",
    "        \n",
    "    # Initialize Athena client\n",
    "    athena_client = boto3.client('athena', \n",
    "                                 region_name=region,\n",
    "                                 aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                                 aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "    # Execute the query\n",
    "    try:\n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            QueryExecutionContext={\n",
    "                'Database': database\n",
    "            },\n",
    "            ResultConfiguration={\n",
    "                'OutputLocation': 's3://prymal-ops/athena_query_results/'  # Specify your S3 bucket for query results\n",
    "            }\n",
    "        )\n",
    "\n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "\n",
    "        # Wait for the query to complete\n",
    "        state = 'RUNNING'\n",
    "        logger.info(f'Running query..')\n",
    "\n",
    "        while (state in ['RUNNING', 'QUEUED']):\n",
    "            response = athena_client.get_query_execution(QueryExecutionId = query_execution_id)\n",
    "            \n",
    "            if 'QueryExecution' in response and 'Status' in response['QueryExecution'] and 'State' in response['QueryExecution']['Status']:\n",
    "                # Get currentstate\n",
    "                state = response['QueryExecution']['Status']['State']\n",
    "\n",
    "                if state == 'FAILED':\n",
    "                    logger.error('Query Failed!')\n",
    "                elif state == 'SUCCEEDED':\n",
    "                    logger.info('Query Succeeded!')\n",
    "            \n",
    "\n",
    "        # OBTAIN DATA\n",
    "\n",
    "        # --------------\n",
    "\n",
    "\n",
    "\n",
    "        query_results = athena_client.get_query_results(QueryExecutionId=query_execution_id,\n",
    "                                                MaxResults= 1000)\n",
    "        \n",
    "\n",
    "\n",
    "        # Extract qury result column names into a list  \n",
    "\n",
    "        cols = query_results['ResultSet']['ResultSetMetadata']['ColumnInfo']\n",
    "        col_names = [col['Name'] for col in cols]\n",
    "\n",
    "\n",
    "\n",
    "        # Extract query result data rows\n",
    "        data_rows = query_results['ResultSet']['Rows'][1:]\n",
    "\n",
    "\n",
    "\n",
    "        # Convert data rows into a list of lists\n",
    "        query_results_data = [[r['VarCharValue'] if 'VarCharValue' in r else np.NaN for r in row['Data']] for row in data_rows]\n",
    "\n",
    "\n",
    "\n",
    "        # Paginate Results if necessary\n",
    "        while 'NextToken' in query_results:\n",
    "                query_results = athena_client.get_query_results(QueryExecutionId=query_execution_id,\n",
    "                                                NextToken=query_results['NextToken'],\n",
    "                                                MaxResults= 1000)\n",
    "\n",
    "\n",
    "\n",
    "                # Extract quuery result data rows\n",
    "                data_rows = query_results['ResultSet']['Rows'][1:]\n",
    "\n",
    "\n",
    "                # Convert data rows into a list of lists\n",
    "                query_results_data.extend([[r['VarCharValue'] if 'VarCharValue' in r else np.NaN for r in row['Data']] for row in data_rows])\n",
    "\n",
    "\n",
    "\n",
    "        results_df = pd.DataFrame(query_results_data, columns = col_names)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "\n",
    "    except ParamValidationError as e:\n",
    "        logger.error(f\"Validation Error (potential SQL query issue): {e}\")\n",
    "        # Handle invalid parameters in the request, such as an invalid SQL query\n",
    "\n",
    "    except WaiterError as e:\n",
    "        logger.error(f\"Waiter Error: {e}\")\n",
    "        # Handle errors related to waiting for query execution\n",
    "\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        error_message = e.response['Error']['Message']\n",
    "        \n",
    "        if error_code == 'InvalidRequestException':\n",
    "            logger.error(f\"Invalid Request Exception: {error_message}\")\n",
    "            # Handle issues with the Athena request, such as invalid SQL syntax\n",
    "            \n",
    "        elif error_code == 'ResourceNotFoundException':\n",
    "            logger.error(f\"Resource Not Found Exception: {error_message}\")\n",
    "            # Handle cases where the database or query execution does not exist\n",
    "            \n",
    "        elif error_code == 'AccessDeniedException':\n",
    "            logger.error(f\"Access Denied Exception: {error_message}\")\n",
    "            # Handle cases where the IAM role does not have sufficient permissions\n",
    "            \n",
    "        else:\n",
    "            logger.error(f\"Athena Error: {error_code} - {error_message}\")\n",
    "            # Handle other Athena-related errors\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Other Exception: {str(e)}\")\n",
    "        # Handle any other unexpected exceptions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query shopify line item data from data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATABASE = 'prymal-analytics'\n",
    "REGION = 'us-east-1'\n",
    "\n",
    "# QUERY ATHENA\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "QUERY = f\"\"\"SELECT a.*\n",
    "            , b.sku_name\n",
    "            , b.product_category\n",
    "            , b.product_type\n",
    "            FROM \"prymal\".\"shopify_line_items\"  a\n",
    "            LEFT JOIN \"prymal\".\"skus_shopify\" b\n",
    "            ON a.sku = b.sku \n",
    "            \n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "# Query datalake to get quantiy sold per sku for the last 120 days\n",
    "# ----\n",
    "\n",
    "result_df = run_athena_query(query=QUERY, database=DATABASE, region=REGION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Data & Calculate Retention Rate by Cohort Month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hero SKU(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hero SKU (or sku to compare users that did / did not have in their first order)\n",
    "hero_sku = ['Variety Pack - Kickstart']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by whether first order contained hero SKU(s), calculate retention rate by first order month cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of athena query results\n",
    "data = result_df.copy()\n",
    "\n",
    "# Convert 'order_date' to datetime\n",
    "data['order_date'] = pd.to_datetime(data['order_date'].apply(lambda x: x[:11]))   # for cases where order_date is datatime, only use first 11 characters (YYYY-MM-DD)\n",
    "\n",
    "\n",
    "# Identify First Orders\n",
    "first_order = data.groupby('email',as_index=False)['order_date'].min()\n",
    "first_order.columns = ['email', 'first_order_date']\n",
    "\n",
    "first_order['cohort_month'] = pd.to_datetime(first_order['first_order_date']).dt.strftime('%Y-%m')\n",
    "\n",
    "print(f'Length of first_order: {len(first_order)} ')\n",
    "\n",
    "# Merge this information back to the main data\n",
    "merged_data = pd.merge(data, first_order, on='email')\n",
    "\n",
    "# Classify Customers Based on Hero SKU\n",
    "merged_data['first_order_contains_hero_sku'] = False\n",
    "merged_data.loc[(merged_data['product_type'].isin(hero_sku))&(merged_data['order_date']==merged_data['first_order_date']),'first_order_contains_hero_sku'] = True\n",
    "\n",
    "\n",
    "# Identify Repeat Purchases\n",
    "customer_purchases = merged_data.groupby('email').apply(\n",
    "    lambda x: (x['order_date'] > x['first_order_date']).any()).reset_index()\n",
    "customer_purchases.columns = ['email', 'made_subsequent_purchase']\n",
    "\n",
    "\n",
    "\n",
    "# Prepare a DataFrame to store the results\n",
    "retention_rate_by_cohort_df = pd.DataFrame(columns=['Cohort', 'Retention Rate', 'Group'])\n",
    "\n",
    "# Calculate retention rate for each cohort's customers\n",
    "for month in merged_data['cohort_month'].unique():\n",
    "    period_data = merged_data[merged_data['cohort_month'] == month]\n",
    "\n",
    "    # Repeat the classification and retention rate calculation for the period\n",
    "    first_order_period = period_data.groupby('email')['first_order_date'].min().reset_index()\n",
    "    customer_classification_period = pd.merge(first_order_period, customer_purchases, on='email')\n",
    "    customer_classification_period = pd.merge(customer_classification_period, \n",
    "                                              period_data[['email', 'first_order_contains_hero_sku']].drop_duplicates(), \n",
    "                                              on='email')\n",
    "\n",
    "    # Calculate retention rate for customers who had hero sku in first order \n",
    "    retention_rate_hero_sku = customer_classification_period[\n",
    "        customer_classification_period['first_order_contains_hero_sku']]['made_subsequent_purchase'].mean()\n",
    "    \n",
    "    # Calculate retention rate for customers who did not have hero sku in first order \n",
    "    retention_rate_no_hero_sku = customer_classification_period[\n",
    "        ~customer_classification_period['first_order_contains_hero_sku']]['made_subsequent_purchase'].mean()\n",
    "\n",
    "    # Append results to DataFrame\n",
    "    retention_rate_by_cohort_df = retention_rate_by_cohort_df.append({'Cohort': month, 'Retention Rate': retention_rate_hero_sku, 'Group': 'Hero SKU'}, ignore_index=True)\n",
    "    retention_rate_by_cohort_df = retention_rate_by_cohort_df.append({'Cohort': month, 'Retention Rate': retention_rate_no_hero_sku, 'Group': 'No Hero SKU'}, ignore_index=True)\n",
    "\n",
    "# Convert 'Period' to string for Plotly\n",
    "retention_rate_by_cohort_df['Cohort'] = pd.to_datetime(retention_rate_by_cohort_df['Cohort']).dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Retention Rate by Cohort Month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Kickstart Bundle Specifically, split into old kickstarter bundle and new (sachet) kickstarter bundle based on order date - new sachet kickstarter bundle replaced old kickstarter bundle 2022-12-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Months < 2022-12 are old kickstarter bundle, > 2022-12 are sachet kickstarter bundle\n",
    "retention_rate_by_cohort_df.loc[(retention_rate_by_cohort_df['Group']=='Hero SKU')&(pd.to_datetime(retention_rate_by_cohort_df['Cohort'])<pd.to_datetime('2022-11-30')),'Group'] = 'Hero SKU - Old Kickstarter Bundle'\n",
    "retention_rate_by_cohort_df.loc[(retention_rate_by_cohort_df['Group']=='Hero SKU'),'Group'] = 'Hero SKU - Sachet Kickstarter Bundle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot observed retention rate by cohort over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Plotly Express line plot\n",
    "fig = px.line(retention_rate_by_cohort_df.sort_values('Cohort',ascending=True), x='Cohort', y='Retention Rate', color='Group', title='First Time Customer Retention Rate Over Time')\n",
    "\n",
    "fig.update_xaxes(type='category',title_text='Cohort Month')\n",
    "\n",
    "# Update y-axis to display as percentage\n",
    "fig.update_layout(yaxis_tickformat = '.0%')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for statistical significane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format data for chi-square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile customer-level classification\n",
    "customer_classification = pd.merge(first_order, customer_purchases, on='email')\n",
    "customer_classification = pd.merge(customer_classification, \n",
    "                                   merged_data[['email', 'first_order_contains_hero_sku']].drop_duplicates(), \n",
    "                                   on='email')\n",
    "\n",
    "# Subset to just 2023 (excluding 2024 through February to avoid customers who haven't had enough time to repeat)\n",
    "customer_classification_2023 = customer_classification.loc[(customer_classification['first_order_date']>pd.to_datetime('2023-01-01'))&(customer_classification['first_order_date']<pd.to_datetime('2024-01-01'))].copy()\n",
    "\n",
    "# Months < 2022-12 are old kickstarter bundle, > 2022-12 are sachet kickstarter bundle\n",
    "customer_classification_2023.loc[(customer_classification_2023['first_order_contains_hero_sku']==False),'Group'] = 'No Hero SKU'\n",
    "customer_classification_2023.loc[(customer_classification_2023['first_order_contains_hero_sku']==True) & (pd.to_datetime(customer_classification_2023['first_order_date'])< pd.to_datetime('2022-12-01')),'Group'] = 'Hero SKU - Old Kickstarter Bundle'\n",
    "customer_classification_2023.loc[(customer_classification_2023['first_order_contains_hero_sku']==True)& (pd.to_datetime(customer_classification_2023['first_order_date'])>= pd.to_datetime('2022-12-01')),'Group'] = 'Hero SKU - Sachet Kickstarter Bundle'\n",
    "\n",
    "# Aggregate results for each group\n",
    "results_by_group = customer_classification_2023.groupby(['Group','made_subsequent_purchase'],as_index=False)['email'].nunique()\n",
    "\n",
    "# Pivot the data to create a contingency table\n",
    "contingency_table = results_by_group.pivot_table(index='Group', columns='made_subsequent_purchase', values='email', aggfunc='sum').reset_index()\n",
    "\n",
    "# Drop the 'Group' column to match the format needed for chi2_contingency\n",
    "contingency_matrix = contingency_table.drop('Group', axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-square test for independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "hero_sku_group_rate = contingency_matrix[0][1] / contingency_matrix[0][0]\n",
    "no_hero_sku_group_rate = contingency_matrix[1][1] / contingency_matrix[1][0]\n",
    "\n",
    "print(f\"Hero SKU population observed retention rate: {hero_sku_group_rate}\")\n",
    "print(f\"No-Hero SKU population observed retention rate: {no_hero_sku_group_rate}\")\n",
    "\n",
    "# Perform the Chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_matrix)\n",
    "\n",
    "print(f\"Chi-square value: {chi2}\")\n",
    "print(f\"P-value: {p}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(f\"Expected frequencies: \\n{expected}\")\n",
    "\n",
    "# Interpret the p-value\n",
    "alpha = 0.05  # significance level\n",
    "if p < alpha:\n",
    "    print(\"There is a significant association between the group and retention status (reject H0).\")\n",
    "else:\n",
    "    print(\"There is no significant association between the group and retention status (fail to reject H0).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
